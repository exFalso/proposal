\documentclass[a4paper]{article}

\usepackage{amsmath,amsfonts}
\usepackage{proof}
%% \usepackage{stmaryrd}
%% \usepackage{etoolbox}
%% \usepackage[hidelinks]{hyperref}
%% \usepackage{pdfcomment}
%% \usepackage{listings}
\usepackage{haskell}

\linespread{1.05}

\oddsidemargin 0in
\evensidemargin 0in
\textheight 9.5in
\textwidth 6.2in
\topmargin -7mm
\parindent 10pt

\begin{document}

\title{Proposal[draft]: Optimization by symbolic evaluation based program construction }
\maketitle
\centerline{
  Andras Slemmer
}

\newcommand{\reduces}{\Downarrow}
\newcommand{\symweak}{\mathbb{W}}
\newcommand{\app}{\ }
\newcommand{\Wrapper}{\text{Wrapper}}
\newcommand{\Worker}{\text{Worker}}

\section{Overview}

\subsection{Idea}

Traditional forward and backward analysis based optimizations do the following:

\begin{itemize}

\item Take a piece of code (e.g. function body) $C$ and perform the given analysis, resulting in some information $I$

\item Use $I$ to transform (and justify transforming) $C$ into more efficient code $C'$

\end{itemize}

The project's aim is to explore the feasibility of gaining enough information in $I$ to enable the \emph{construction} of some new code $D$ that has the same semantics as $C$. The idea is to do this by capturing the \emph{effect} of the reduction of $C$ by doing symbolic evaluation, extracting the semantically relevant parts of it and constructing code that produces that same effect. The idea is that by doing so we can get rid of semantically unnecessary operations that may be present in $C$ without having to recourse to specific types of analyses.

\subsection{Motivation}

\subsubsection*{Subsumption of existing optimizations}

The optimization would therefore have two parts: symbolic evaluation (code $\to$ effect) and construction (effect $\to$ code), and it can be tuned by changing what the input is and what we do with the effect before construction.

Existing optimizations like strictness analysis/demand analysis, inlining or compile time garbage collection can be seen as different parameterizations.

\begin{itemize}
\item Strictness/demand analysis

Strictness analysis allows compilers of lazy languages to detect possibilities of eager argument evaluation. This has the benefit that it allows the compiler to avoid ``boxing'' the value(allocating a thunk for it in the heap) and pass the value directly.

We can detect these opportunities by doing symbolic evaluation that calculates what terms will definitely be reduced in the expression(code $\to$ effect). Then we can construct code that does precisely the same reductions, only this time we are in control of when and in what order these should happen for maximum efficienct. We will explore this idea later a bit more formally.

(Cite Mycroft (Cousots?)(strictness), Peyton Jones(demand).)

\item Inlining

Because we are doing symbolic evaluation our reduction will stop and one point or another because of lack of information. An example of this is when we want to reduce a symbol that is not in scope of the evaluation. In these cases all we can do is reconstruct the original reduction. However by simply extending the input with information about symbols in scope we allow the symbolic evaluation to continue, and in effect inline the symbol.

\item Compile time garbage collection

If our runtime is garbage collected we will most probably want an intermediate representation that can directly manipulate the garbage collected heap. The effect of the reduction of this representation is then going to be allocations, dereferences etcetera. What we can do is before we do construction of the new code we can garbage collect the symbolic heap by looking at what references are in scope at the end. This way unneeded heap operations will not be present in the new code.

\end{itemize}

\subsubsection*{Lower coupling of responsibilities in compilers}

Because our optimization constructs code on the same abstraction level as it started from it can be seen as an automorphism on a given intermediate representation. If we were successful in defining a general framework for evaluation-construction then compilation would become a matter of choosing the right intermediate representations until we reach machine code. The compiler writer can focus on the correctness of each translation step without having to worry about generating optimal code. Optimization would become a decoupled operation that may be performed on each representation.

\subsubsection*{Possibility of dynamic optimization}

If we integrate this optimization into the runtime system we can do the same analysis extended with dynamic information about the runtime state (e.g. instead of starting with an unknown heap extend it with available information), possibly resulting in the construction of code that is specifically catered for the runtime environment at hand. An example of this would be a long running program that initially loads a configuration. Statically we have no way of knowing what the configuration is going to be, and we might miss out on chances to prune whole parts of the program (if the configuration affects branching), however at runtime we detect these long-residing objects and inline them dynamically.

\mbox{}\\


\section{Examples of desired results}

In the following I will try to demonstrate what I mean by \emph{effect} and \emph{construction}. Our goal will be to take a slightly modified version of the lazy lambda calculus presented in Launchbury's seminal paper\cite{Launchbury:1993:NSL:158511.158618} and extend it so that it is capable of expressing optimized code in terms of strictness and unboxing.

We will then proceed to give operational semantics for the calculus itself and afterwards define symbolic evaluation that calculates the effect of the reduction, which, unsurprisingly, will closely follow the structure of the operational semantics. Once we have an understanding of the effect we can define the construction of optimized code producing the same effect.

Please note that my intention with this example is to demonstrate the idea rather than to provide an actual solution to the strictness optimization problem.

\subsection{The calculus}

The goal of strictness analysis is to allow strict evaluation of arguments to a lambda and avoid unnecessary creating of thunks in the heap. In order to capture this we need to introduce syntactic constructions that allow explicit control over reduction to normal form and allow argument passing without sharing. We will do this by introducting \emph{strict} and \emph{unboxed} application

\newcommand{\strict}{\ !\ }
\newcommand{\unboxed}{\ \#\ }

\begin{align}
  M,N&::=\ x|\lambda x.M|M \app N|M \unboxed N|M \strict N
\end{align}

$M \unboxed N$ denotes unboxed application, $M \strict N$ denotes strict application.

\subsection{Operational semantics}

In the following we will simply extend the semantics given in \cite{Launchbury:1993:NSL:158511.158618} with one exception: In the paper the semantics of application is actually that of unboxed applicaton, that is, an argument's reduction is not going to be shared if it is used several times in the body of a lambda. Instead we will give semantics of a boxed application, that is, one that creates a new thunk in the heap for the argument. The original semantics will be that of unboxed application. Furthermore we will not consider let bindings and issues with renaming for simplicity.

\begin{gather*}
  \infer
      [\text{(Lambda)}]
      {\Gamma : \lambda x. e \reduces \Gamma : \lambda x. e}
      {}
      \\
      \\
  \infer
      [\text{(Boxed)}]
      {\Gamma : f x \reduces \Gamma'' : z}
      {\Gamma : f \reduces \Gamma' : \lambda y. e \;\;\;\;\; \Gamma'[\nu \to x] : e[\nu/y] \reduces \Gamma'' : z \;\;\;\;\; \text{$\nu$ fresh}}
      \\
      \\
  \infer
      [\text{(Unboxed)}]
      {\Gamma : f \unboxed x \reduces \Gamma'' : z}
      {\Gamma : f \reduces \Gamma' : \lambda y. e \;\;\;\;\; \Gamma' : e[x/y] \reduces \Gamma'' : z}
      \\
      \\
  \infer
      [\text{(Strict)}]
      {\Gamma : f \strict x \reduces \Gamma''' : z}
      {\Gamma : x \reduces \Gamma' : s \;\;\;\;\; \Gamma' : f \reduces \Gamma'' : \lambda y. e \;\;\;\;\; \Gamma'' : e[s/y] \reduces \Gamma''' : z}
      \\
      \\
  \infer
      [\text{(Variable)}]
      {\Gamma[x \to e] : x \reduces \Gamma'[x \to z] : z}
      {\Gamma : e \reduces \Gamma' : z}
\end{gather*}


\subsection{Symbolic reduction}

Before we specify symbolic reduction we need to understand what the \emph{effect} of reduction is. In our case it is going to be the set of variables and applications that are reduced to weak head normal form.

Our reduction context is going to be a set of reductions and a \emph{symbolic} heap. By symbolic I mean that the reduction will finish if we try to reduce a variable that is not in the heap (free variables), much like what happens in the operational semantics in the case of lambdas. This stopping of the symbolic evaluation would be a common theme for any intermediate representation we would like to examine this way, as we can never have complete information about the runtime state.

\mbox{}\\

\begingroup
\leftskip2em Sidenode: Initially I only planned to account for unboxing/strictness opportunities, but I realized that with little modification the analysis can also account for \emph{safe} common subexpression elimination as well. By \emph{safe} I mean that, we only extract two equivalent subexpressions into one if both are guaranteed to reduce. This way we avoid the potential memory leaks caused by a naive CSE.
\par
\endgroup


\mbox{}\\

The reduction set will contain either variables or applications. Note that if during symbolic evaluation we reduce $f \app x$ as well as $f \strict x$ then we want to represent them as the same in the reduction set. This is because at construction we will know whether we want unboxed application (if the argument was reduced as well) or simple application. In essence all strict applications will become unboxed, all unboxed applications will stay unboxed and boxed applications may or may not become unboxed. To this end we will need a helper function to disregard the type of applications in a term, in particular those that are guaranteed to reduce:

\newcommand{\uppity}[1]{#1^{\uparrow}}

\begin{align*}
  \uppity{x} &= x\\
  \uppity{(f \unboxed x)} &= \uppity{f} \app x\\
  \uppity{(f \app x)} &= \uppity{f} \app x\\
  \uppity{(f \strict x)} &= \uppity{f} \app \uppity{x}\\
  \uppity{(\lambda x. M)} &= \lambda x. M
\end{align*}


Our symbolic reductions will have the form $\Gamma, R : M \reduces \Gamma', R' : N$, where $\Gamma$ is the symbolic heap, $R$ is the reduction set and $M$ is the term at hand.

The reductions then are:


\begin{gather*}
  \infer
      [\text{(Lambda)}]
      {\Gamma, R : \lambda x. e \reduces \Gamma, R : \lambda x. e}
      {}
      \\
      \\
  \infer
      [\text{(Variable, known)}]
      {\Gamma[x \to e], R : x \reduces \Gamma'[x \to z], R' : z}
      {\Gamma, R : e \reduces \Gamma', R' : z}
      \\
      \\
  \infer
      [\text{(Variable, unknown)}]
      {\Gamma, R : x \reduces \Gamma, R \cup \{x\} : x}
      {x \notin \Gamma}
      \\
      \\
  \infer
      [\text{(Boxed, known)}]
      {\Gamma,R : f \app x \reduces \Gamma'',R'' : z}
      {\Gamma,R : f \reduces \Gamma',R' : \lambda y. e \;\;\;\;\; \Gamma'[w \to x],R' : e[w/y] \reduces \Gamma'',R'' : z}
      \\
      \\
  \infer
      [\text{(Boxed, unknown)}]
      {\Gamma,R : f \app x \reduces \Gamma',R' \cup \{\uppity{(g \app x)}\} : g \app x}
      {\Gamma,R : f \reduces \Gamma',R' : g \;\;\;\;\; g \neq \lambda \_.\_}
      \\
      \\
  \infer
      [\text{(Unboxed, known)}]
      {\Gamma,R : f \unboxed x \reduces \Gamma'',R'' : z}
      {\Gamma,R : f \reduces \Gamma',R' : \lambda y. e \;\;\;\;\; \Gamma',R' : e[x/y] \reduces \Gamma'',R'' : z}
      \\
      \\
      \infer
      [\text{(Unboxed, unknown)}]
      {\Gamma,R : f \unboxed x \reduces \Gamma',R' \cup \{\uppity{(g \unboxed x)}\} : g \unboxed x}
      {\Gamma,R : f \reduces \Gamma',R' : g \;\;\;\;\; g \neq \lambda \_.\_}
      \\
      \\
  \infer
      [\text{(Strict, known)}]
      {\Gamma, R : f \strict x \reduces \Gamma''', R''' : z}
      {\Gamma, R : x \reduces \Gamma', R' : s \;\;\;\;\; \Gamma', R' : f \reduces \Gamma'', R'' : \lambda y. e \;\;\;\;\; \Gamma'', R'' : e[s/y] \reduces \Gamma''', R''' : z}
      \\
      \\
  \infer
      [\text{(Strict, unknown)}]
      {\Gamma, R : f \strict x \reduces \Gamma'', R'' \cup \{\uppity{(g \strict z)}: g \strict z}
      {\Gamma,R : f \reduces \Gamma',R' : g \;\;\;\;\; g \neq \lambda \_.\_ \;\;\;\;\; \Gamma', R' : x \reduces \Gamma'', R'' : z}
      \\
      \\
\end{gather*}

\subsection{Construction}

\newcommand{\constr}[1]{\{#1\}}

Our construction $\constr{-}_{\rho}$ needs to do the same reductions as the original term and it needs to do these in an order that minimizes work. Because of this we will traverse the Reduction set in an order that has the following constraint:

$\forall M, N. (M \app N) \in R \to (M \in R \to M < (M \app N)) \land (N \in R \to N < (M \app N))$

In other words we want the reduction of both the function and the argument to happen before the reduction of the application does. In the construction we will assume that the input reduction list satisfies this constraint.

The construction will use an environment $\rho$ to keep track of fresh variables corresponding to reduced variables/applications.

\begin{align*}
  \constr{[]}_{\rho, T} &= \rho(T)\\
  \constr{x :: L}_{\rho, T} &= (\lambda \nu. \constr{L})_{\rho[x\to\nu], T} \strict x && \text{$\nu$ fresh}\\
  \constr{M N :: L}_{\rho, T} &= (\lambda \nu. \constr{L})_{\rho[M \app N \to\nu], T} \strict X && \text{$\nu$ fresh}\\
  & \text{where}\\
  & \;\;\; X = \rho(M) \unboxed \rho(N) && \text{if $N \in \rho$}\\
  & \;\;\; X = \rho(M) \app N && \text{otherwise}\\
\end{align*}

\subsection{Example}

To demonstrate what the above transformation does let's examine what happens with the following term:\\

$M = g \app x + (g \app x + x)$\\

$g$ and $x$ are free variables and $+$ is defined as\\

$a + b = plus \strict a \strict b$\\

In other words it is an unknown function (free variable) that requires both arguments to be in whnf.\\

After symbolic reduction and construction (choosing an arbitrary ordering for construction) the new term is going to be:\\

$(\lambda 1.(\lambda 2.(\lambda 3.(\lambda 4.(\lambda 5.(\lambda 6.(\lambda 7.7) \strict (5 \unboxed 6)) \strict (5 \unboxed 2)) \strict (4 \unboxed 3)) \strict plus) \strict (1 \unboxed 2)) \strict x) \strict g$\\

Arguably this is not the most beautiful term ever, the construction function could be made much smarter, but it does demonstrate that the algorithm correctly detected strictness, unboxing and CSE opportunities.

\section{Project plan}

\subsection{Goals}

By the end of the project I would like to

\begin{itemize}
\item Have shown that the proposed optimization can be done and works
\item Have developed a theoretical framework that allows sound implementation of the technique
\item Have a working compiler and runtime
\item Have written a backend for an existing in-use frontend language (e.g. Haskell or ML)
\end{itemize}

\subsection{Steps}

Following is the layout of the bigger strokes of the project. The ordering is somewhat arbitrary and would be fixed as the project develops.

\subsubsection*{Reading}

This part of the project would focus on exploring the relevant parts of compiler research. This includes papers on static analyses, semantics (e.g. abstract interpretation) and symbolic evaluation. (Papers/theses in mind?)

\subsubsection*{Toy compiler}

My plan is to develop a toy compiler for experimenting purposes. I very much intend the project to have practical results, and a working implementation of the sketched optimization technique is a good coherence criterion for the idea anyway.

\subsubsection*{Theoretical basis}

This would involve developing a rigorous theoretical framework in which we can examine the soundness of the optimization, as well as give optimization conditions(e.g. we are guaranteed to have less memory allocations than before). My goal is to develop a theory general enough to be agnostic with regards to the language.

The research would initially focus on a language with limited capabilities e.g. lazy lambda calculus with primitive numbers or an imperative garbage collected language working on the heap.

\subsubsection*{More expressiveness}

Once we are done with the simple languages we can move onto more muddy waters and look at how to express the effect of concurrency or side effects.

For example we can represent side effecting function calls as a chain of states the runtime must be in when making the calls. Construction then needs to make sure that these states are indeed reached and the ordering is preserved.

Concurrency we may want to represent as a set of constraints on the interleaving of evaluations which the construction needs to satisfy.

\subsubsection*{Dynamic optimization}

This part would be about how to incorporate the optimization into the runtime. An example use case would be detecting long residing objects in the heap and specializing closures that use it. This requires a couple of things:

The runtime must use a representation of both the code and the runtime state that is amenable to symbolic evaluation. This suggests that we either use a bytecode intepreter that is used both for execution and analysis, or think of a way to abstract the runtime state into a higher level representation.

Furthermore the runtime should be able to determine whether such an optimization is feasible at all. The above example is to detect long residing objects, but it may also be the detection of ``hot'' code (JVM style).

\subsubsection*{Backend for existing language}

This is what would complete the practical aspect of the project. As the optimization technique is frontend-language-agnostic we can pick any language that has clear semantics. We could even bootstrap the compiler itself by targeting the language it was written in.

\bibliographystyle{unsrt}
\bibliography{Proposal}

\end{document}
